{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Network from Scratch\n",
    "\n",
    "This notebook implements a 4-layer deep neural network using only NumPy for binary image classification (cat vs non-cat).\n",
    "\n",
    "## Architecture\n",
    "\n",
    "```\n",
    "Input (12288) → Dense (20, ReLU) → Dense (7, ReLU) → Dense (5, ReLU) → Output (1, Sigmoid)\n",
    "```\n",
    "\n",
    "## Key Components\n",
    "\n",
    "- **Forward propagation**: Vectorized computation through all layers\n",
    "- **Backward propagation**: Gradient computation using chain rule\n",
    "- **Optimization**: Gradient descent with configurable learning rate\n",
    "- **Activation functions**: ReLU (hidden layers), Sigmoid (output layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "from PIL import Image\n",
    "from utils import *\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0)\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Explore Dataset\n",
    "\n",
    "The dataset consists of 64×64 RGB images labeled as cat (1) or non-cat (0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_orig, train_y, test_x_orig, test_y, classes = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a sample image\n",
    "index = 10\n",
    "plt.imshow(train_x_orig[index])\n",
    "label = classes[np.squeeze(train_y[:, index])].decode(\"utf-8\")\n",
    "plt.title(f\"Label: {label}\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset dimensions\n",
    "m_train = train_x_orig.shape[0]\n",
    "m_test = test_x_orig.shape[0]\n",
    "num_px = train_x_orig.shape[1]\n",
    "\n",
    "print(f\"Training examples: {m_train}\")\n",
    "print(f\"Test examples: {m_test}\")\n",
    "print(f\"Image dimensions: {num_px}×{num_px}×3\")\n",
    "print(f\"train_x_orig shape: {train_x_orig.shape}\")\n",
    "print(f\"train_y shape: {train_y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocess Data\n",
    "\n",
    "Reshape images from (m, 64, 64, 3) to (12288, m) and normalize pixel values to [0, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten images: (m, 64, 64, 3) → (64*64*3, m)\n",
    "train_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T\n",
    "test_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T\n",
    "\n",
    "# Normalize pixel values to [0, 1]\n",
    "train_x = train_x_flatten / 255.0\n",
    "test_x = test_x_flatten / 255.0\n",
    "\n",
    "print(f\"train_x shape: {train_x.shape}\")\n",
    "print(f\"test_x shape: {test_x.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define Network Architecture\n",
    "\n",
    "The network has 4 layers:\n",
    "- Input: 12,288 features (64×64×3 flattened)\n",
    "- Hidden 1: 20 neurons (ReLU)\n",
    "- Hidden 2: 7 neurons (ReLU)\n",
    "- Hidden 3: 5 neurons (ReLU)\n",
    "- Output: 1 neuron (Sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_dims = [train_x.shape[0], 20, 7, 5, 1]  # 4-layer model\n",
    "print(f\"Network architecture: {layers_dims}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Build the Model\n",
    "\n",
    "The training loop performs:\n",
    "1. Forward propagation to compute predictions\n",
    "2. Cost computation (binary cross-entropy)\n",
    "3. Backward propagation to compute gradients\n",
    "4. Parameter updates via gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_model(X, Y, layers_dims, learning_rate=0.0075, num_iterations=3000, print_cost=False):\n",
    "    \"\"\"\n",
    "    Implements an L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "        X: input data of shape (n_x, m)\n",
    "        Y: labels of shape (1, m)\n",
    "        layers_dims: list containing layer dimensions\n",
    "        learning_rate: learning rate for gradient descent\n",
    "        num_iterations: number of iterations for optimization\n",
    "        print_cost: if True, print cost every 100 iterations\n",
    "    \n",
    "    Returns:\n",
    "        parameters: learned parameters (weights and biases)\n",
    "        costs: list of costs during training\n",
    "    \"\"\"\n",
    "    np.random.seed(1)\n",
    "    costs = []\n",
    "\n",
    "    # Initialize parameters\n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "\n",
    "    # Gradient descent loop\n",
    "    for i in range(num_iterations):\n",
    "        # Forward propagation\n",
    "        AL, caches = L_model_forward(X, parameters)\n",
    "\n",
    "        # Compute cost\n",
    "        cost = compute_cost(AL, Y)\n",
    "\n",
    "        # Backward propagation\n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    "\n",
    "        # Update parameters\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "\n",
    "        # Record cost\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "            if print_cost:\n",
    "                print(f\"Cost after iteration {i}: {cost:.6f}\")\n",
    "\n",
    "    return parameters, costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters, costs = nn_model(\n",
    "    train_x, \n",
    "    train_y, \n",
    "    layers_dims, \n",
    "    learning_rate=0.0075, \n",
    "    num_iterations=2500, \n",
    "    print_cost=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning curve\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(costs)\n",
    "plt.ylabel('Cost')\n",
    "plt.xlabel('Iterations (per 100)')\n",
    "plt.title('Training Loss Curve')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Set:\")\n",
    "pred_train = predict(train_x, train_y, parameters)\n",
    "\n",
    "print(\"\\nTest Set:\")\n",
    "pred_test = predict(test_x, test_y, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualize Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_predictions(X_orig, y_true, y_pred, classes, num_samples=5):\n",
    "    \"\"\"Display sample predictions with their true labels.\"\"\"\n",
    "    fig, axes = plt.subplots(1, num_samples, figsize=(15, 3))\n",
    "    \n",
    "    for i, ax in enumerate(axes):\n",
    "        ax.imshow(X_orig[i])\n",
    "        true_label = classes[int(y_true[0, i])].decode(\"utf-8\")\n",
    "        pred_label = classes[int(y_pred[0, i])].decode(\"utf-8\")\n",
    "        color = 'green' if true_label == pred_label else 'red'\n",
    "        ax.set_title(f\"True: {true_label}\\nPred: {pred_label}\", color=color)\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "show_predictions(test_x_orig, test_y, pred_test, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Test on Custom Image (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(image_path, parameters):\n",
    "    \"\"\"Predict whether an image contains a cat.\"\"\"\n",
    "    # Load and preprocess image\n",
    "    img = Image.open(image_path).resize((64, 64))\n",
    "    img_array = np.array(img).reshape(1, -1).T / 255.0\n",
    "    \n",
    "    # Forward propagation\n",
    "    AL, _ = L_model_forward(img_array, parameters)\n",
    "    prediction = (AL > 0.5).astype(int)\n",
    "    \n",
    "    # Display result\n",
    "    plt.imshow(img)\n",
    "    label = \"cat\" if prediction[0, 0] == 1 else \"non-cat\"\n",
    "    confidence = AL[0, 0] if prediction[0, 0] == 1 else 1 - AL[0, 0]\n",
    "    plt.title(f\"Prediction: {label} ({confidence:.1%} confidence)\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Uncomment to test with your own image:\n",
    "# predict_image(\"path/to/your/image.jpg\", parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrates a complete deep learning pipeline built from scratch:\n",
    "\n",
    "| Component | Implementation |\n",
    "|-----------|---------------|\n",
    "| Forward propagation | Vectorized matrix operations |\n",
    "| Activation functions | ReLU (hidden), Sigmoid (output) |\n",
    "| Cost function | Binary cross-entropy |\n",
    "| Backward propagation | Chain rule gradient computation |\n",
    "| Optimization | Gradient descent |\n",
    "\n",
    "The model achieves reasonable accuracy on this small dataset, demonstrating that the fundamental building blocks work correctly. For production use, consider using frameworks like PyTorch or TensorFlow with pretrained models for better performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
